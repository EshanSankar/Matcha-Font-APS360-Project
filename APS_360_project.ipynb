{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCxD3KrOQOeU"
      },
      "source": [
        "# APS360 Group Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2TNmulY7lunm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUTmcXnAkDVT"
      },
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "FONT_DATASET_PATH = \"./fonts_image_dataset\"\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \n",
        "    # Use the default collate function to batch the data (images)\n",
        "    batch = default_collate(batch)\n",
        "    images, labels = batch\n",
        "    \n",
        "    # Apply one-hot encoding to the labels\n",
        "    labels = F.one_hot(labels)\n",
        "    \n",
        "    return images, labels\n",
        "\n",
        "def load_dataset(dataset_path, batch_size):\n",
        "\n",
        "    # Convert the images to tensors and normalize them\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Grayscale(num_output_channels=1)])\n",
        "    gestures_dataset = torchvision.datasets.ImageFolder(root = dataset_path, transform=transform)\n",
        "\n",
        "    # Create a list of indices for all the images in the dataset\n",
        "    dataset_size = len(gestures_dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Split the indices into 60% Training 20% Validation 20% Testing. We need most of the data for training the network, but we must also set aside a bit for validation to fine tune the network, and test the network at the very end.\n",
        "    split1 = int(0.6 * dataset_size)\n",
        "    split2 = int(0.8 * dataset_size)\n",
        "    train_indices, val_indices, test_indices = indices[:split1], indices[split1:split2], indices[split2:]\n",
        "\n",
        "    # Create a sampler for the training, validation, and testing sets\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # Create the dataloaders for the training, validation, and testing sets\n",
        "    train_loader = torch.utils.data.DataLoader(gestures_dataset, batch_size=batch_size,sampler=train_sampler,collate_fn=custom_collate_fn)\n",
        "    val_loader = torch.utils.data.DataLoader(gestures_dataset, batch_size=batch_size,sampler=val_sampler,collate_fn=custom_collate_fn)\n",
        "    test_loader = torch.utils.data.DataLoader(gestures_dataset, batch_size=batch_size,sampler=test_sampler,collate_fn=custom_collate_fn)\n",
        "\n",
        "    print(\"Done Loading Data\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, gestures_dataset.classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iojl5j1j85J"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tY_SOfBvQFya"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__() \n",
        "        self.conv1 = nn.Conv2d(1, 5, 3) \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 3)\n",
        "        self.conv3 = nn.Conv2d(10, 20, 3)\n",
        "        self.fc = nn.Linear(26*26*20, 10)\n",
        "\n",
        "    def forward(self, x, training=False):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 26*26*20)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mBeEgPlCcg"
      },
      "source": [
        "## Training & Output Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def total_error(outputs, labels):\n",
        "    \n",
        "    # Find the indices of the max values\n",
        "    _, indices = torch.max(outputs, dim=1, keepdim=True)\n",
        "\n",
        "    # Create a tensor of zeros with the same shape as x\n",
        "    zeros = torch.zeros_like(outputs)\n",
        "\n",
        "    # Set the max values to 1\n",
        "    zeros.scatter_(1, indices, 1)\n",
        "    \n",
        "    return (zeros != labels).any(dim=1).float().sum()\n",
        "\n",
        "def evaluate(net, loader, criterion):\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_epoch = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(loader, 0):\n",
        "                \n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate the statistics\n",
        "            total_err += total_error(outputs, labels)\n",
        "            total_loss += criterion(outputs, labels.float()).item()\n",
        "            total_epoch += len(labels)\n",
        "\n",
        "    err = float(total_err) / total_epoch\n",
        "    loss = float(total_loss) / (i + 1)\n",
        "\n",
        "    return err, loss\n",
        "\n",
        "\n",
        "def train_net(net, model_name, dataset_path = FONT_DATASET_PATH, batch_size=128, learning_rate=0.01, num_epochs=30):\n",
        "\n",
        "    # Create the directory to store model if it does not exist\n",
        "    if not os.path.exists(model_name):\n",
        "      os.makedirs(model_name)\n",
        "    \n",
        "    # Set the seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Load the data\n",
        "    train_loader, val_loader, test_loader, classes = load_dataset(dataset_path, batch_size)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Set up some numpy arrays to store the loss/error rate\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    \n",
        "    min_validation_error = 1\n",
        "    \n",
        "    print(\"Starting Training\")\n",
        "    \n",
        "    # Train the network\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "\n",
        "            net.train()\n",
        "            \n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs, training = True)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            net.eval()\n",
        "            \n",
        "            # Calculate the statistics\n",
        "            total_train_err += total_error(outputs, labels)\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "        \n",
        "        # Store the statistics in the numpy arrays\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "        \n",
        "        # Print the statistics\n",
        "        print(f\"Epoch {epoch + 1}: Train err: {train_err[epoch]}, Train loss: {train_loss[epoch]} | Validation err: {val_err[epoch]}, Validation loss: {val_loss[epoch]}\")\n",
        "        \n",
        "        # Save the best model\n",
        "        if train_err[epoch] <= min_validation_error:\n",
        "            min_validation_error = train_err[epoch]\n",
        "            torch.save(net.state_dict(), f\"{model_name}/best_baseline_model\")\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # Write the loss/err into CSV file for plotting later\n",
        "    np.savetxt(f\"{model_name}/train_err.csv\", train_err)\n",
        "    np.savetxt(f\"{model_name}/train_loss.csv\", train_loss)\n",
        "    np.savetxt(f\"{model_name}/val_err.csv\", val_err)\n",
        "    np.savetxt(f\"{model_name}/val_loss.csv\", val_loss)\n",
        "\n",
        "def plot_training_curve(path):\n",
        "    train_err = np.loadtxt(\"{}/train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}/val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}/train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}/val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    num_epochs = len(train_err)\n",
        "    plt.plot(range(1,num_epochs+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,num_epochs+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,num_epochs+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,num_epochs+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dX2REwQIyX_R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Loading Data\n",
            "Starting Training\n",
            "Epoch 1: Train err: 0.905, Train loss: 6.953918218612671 | Validation err: 0.895, Validation loss: 2.3938710689544678\n",
            "Epoch 2: Train err: 0.8966666666666666, Train loss: 2.351216173171997 | Validation err: 0.875, Validation loss: 2.3011850118637085\n",
            "Epoch 3: Train err: 0.895, Train loss: 2.2979176521301268 | Validation err: 0.915, Validation loss: 2.3006064891815186\n",
            "Epoch 4: Train err: 0.8883333333333333, Train loss: 2.2822594165802004 | Validation err: 0.87, Validation loss: 2.2913042306900024\n",
            "Epoch 5: Train err: 0.855, Train loss: 2.294100761413574 | Validation err: 0.905, Validation loss: 2.2968597412109375\n",
            "Epoch 6: Train err: 0.805, Train loss: 2.2634339809417723 | Validation err: 0.94, Validation loss: 2.3990198373794556\n",
            "Epoch 7: Train err: 0.84, Train loss: 2.2278629779815673 | Validation err: 0.96, Validation loss: 2.416912317276001\n",
            "Epoch 8: Train err: 0.81, Train loss: 2.1879334449768066 | Validation err: 0.96, Validation loss: 2.4197027683258057\n"
          ]
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(0)\n",
        "net = BaselineModel()\n",
        "train_net(net, \"baseline_model\", learning_rate=0.01, num_epochs=15)\n",
        "plot_training_curve(\"baseline_model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
