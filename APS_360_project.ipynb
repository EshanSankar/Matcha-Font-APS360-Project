{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCxD3KrOQOeU"
      },
      "source": [
        "# APS360 Group Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2TNmulY7lunm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from train import train_net, train_auto_encoder, init_device, test_net\n",
        "from graph import plot_training_curve, generate_confusion_matrix, visualize_output, visualize_autoencoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda Available: False\n"
          ]
        }
      ],
      "source": [
        "init_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iojl5j1j85J"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tY_SOfBvQFya"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__() \n",
        "        self.conv1 = nn.Conv2d(1, 5, 3) \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 3)\n",
        "        self.conv3 = nn.Conv2d(10, 20, 3)\n",
        "        self.fc = nn.Linear(26*26*20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 26*26*20)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dX2REwQIyX_R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Loading Data\n",
            "Starting Training\n",
            "Epoch 1: Train err: 0.6415, Train loss: 1.7101116281874635 | Validation err: 1.0, Validation loss: 10.43777847290039\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaselineModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\samue\\Documents\\Code\\VS Code\\Python\\APS360 Project\\train.py:180\u001b[0m, in \u001b[0;36mtrain_net\u001b[1;34m(model_class, model_name, model_params, dataset_path, batch_size, learning_rate, num_epochs, patience)\u001b[0m\n\u001b[0;32m    178\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    183\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_net(BaselineModel, \"baseline_model\", learning_rate=0.01, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 2, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, 2, 1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(F.relu(self.conv1(x)))    # 32 x 112 x 112\n",
        "        x = self.bn2(F.relu(self.conv2(x)))    # 64 x 56 x 56\n",
        "        x = self.bn3(F.relu(self.conv3(x)))    # 128 x 28 x 28\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.conv4 = nn.ConvTranspose2d(128, 64, 3, 2, 1, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.conv5 = nn.ConvTranspose2d(64, 32, 3, 2, 1, 1)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.conv6 = nn.ConvTranspose2d(32, 1, 3, 2, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.bn4(F.relu(self.conv4(x)))\n",
        "        x = self.bn5(F.relu(self.conv5(x)) )\n",
        "        x = self.conv6(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PrimaryModel(nn.Module):\n",
        "    def __init__(self, encoder, layer_size):\n",
        "        super(PrimaryModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.encoder.requires_grad_(False)\n",
        "        self.layer_size = layer_size\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, layer_size, 3, 2, 1) \n",
        "        self.bn1 = nn.BatchNorm2d(layer_size)\n",
        "        self.conv2 = nn.Conv2d(layer_size, layer_size*2, 3, 2, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(layer_size*2)\n",
        "        self.conv3 = nn.Conv2d(layer_size*2, layer_size*4, 3, 2, 1)\n",
        "        self.bn3 = nn.BatchNorm2d(layer_size*4)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, layer_size*4, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(layer_size*4)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(layer_size*8,layer_size*2,1)\n",
        "        self.bn5 = nn.BatchNorm2d(layer_size*2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(layer_size*2*28*28, 128)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.bn1(F.relu(self.conv1(x)))\n",
        "        x1 = self.bn2(F.relu(self.conv2(x1)))\n",
        "        x1 = self.bn3(F.relu(self.conv3(x1)))\n",
        "        \n",
        "        x2 = self.encoder(x)\n",
        "        x2 = self.bn4(F.relu(self.conv4(x2)))\n",
        "        \n",
        "        x = torch.cat((x1,x2), dim=1)\n",
        "        x = self.bn5(F.relu(self.conv5(x)))\n",
        "                \n",
        "        x = x.view(-1, self.layer_size*2*28*28)\n",
        "        x = self.bn6(F.relu(self.fc1(x)))\n",
        "        x = F.softmax(self.fc2(x), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_auto_encoder(AutoEncoder, \"autoencoder\", learning_rate=0.002, batch_size=16, patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for num_layers in range(1, 9):\n",
        "    auto_encoder = AutoEncoder()\n",
        "    auto_encoder.load_state_dict(torch.load(\"autoencoder\\\\best_model\"))\n",
        "    train_net(PrimaryModel, \"primary_model\" + str(num_layers), model_params=[auto_encoder.encoder, num_layers], learning_rate=0.0001, batch_size=32, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model: Done Loading Data\n",
            "Test error: 1.0, Test loss: 9.777180314064026\n",
            "Primary Model: "
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'PrimaryModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m test_net(BaselineModel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimary Model: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m test_net(\u001b[43mPrimaryModel\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_params\u001b[38;5;241m=\u001b[39m[Encoder()])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'PrimaryModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "print(\"Baseline Model: \", end=\"\")\n",
        "test_net(BaselineModel, \"baseline_model\\\\best_model\")\n",
        "print(\"Primary Model: \", end=\"\")\n",
        "test_net(PrimaryModel, \"primary_model\\\\best_model\", model_params=[Encoder()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graphing\n",
        "plot_training_curve(\"baseline_model\")\n",
        "plot_training_curve(\"primary_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing\n",
        "visualize_output(5 ,\"baseline_model\\\\best_model\", BaselineModel)\n",
        "visualize_output(5, \"primary_model\\\\best_model\", PrimaryModel, model_params=[Encoder()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_autoencoder_output(5, \"autoencoder\\\\best_model\", AutoEncoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "generate_confusion_matrix(BaselineModel, \"baseline_model\\\\best_model\")\n",
        "generate_confusion_matrix(PrimaryModel, \"primary_model\\\\best_model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
